{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the required libraries and installing it if doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all relevant libraries for use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "\n",
    "pyodbc.pooling = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating files in their order of execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\OlistPrimaryDataSet\\\\'\n",
    "\n",
    "olist_source_files = os.listdir(path)\n",
    "base_tbls = []\n",
    "intermediate_tbls = []\n",
    "dependent_tbls = []\n",
    "\n",
    "for file in olist_source_files:\n",
    "    file_name = (file.split(\".\")[0]).upper()\n",
    "    if file_name in ['OLIST_PRODUCTS','OLIST_SELLERS', 'OLIST_CUSTOMERS', 'OLIST_GEOLOCATION', 'PRODUCT_CATEGORY_NAME_TRANSLATION']:\n",
    "        base_tbls.append(file)\n",
    "    elif file_name in ['OLIST_ORDERS']:\n",
    "        intermediate_tbls.append(file)\n",
    "    else:\n",
    "        dependent_tbls.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_query(table_name, columns, number_of_columns):\n",
    "    ''' Constructs a SQL INSERT query.\n",
    "    \n",
    "        Args:\n",
    "            table_name: Name of the table to be inserted.\n",
    "            columns: A sequence of strings representing the column name in database.\n",
    "            number_of_columns: Numbers of columns in the table\n",
    "\n",
    "        Returns:\n",
    "            A string which represents a SQL INSERT query. For example:\n",
    "\n",
    "            INSERT INTO Customers (CustomerName, ContactName, Address, City, PostalCode, Country)\n",
    "            VALUES (?, ?, ?, ?, ?, ?);'''\n",
    "    if table_name and columns and number_of_columns:\n",
    "        sql = '''INSERT INTO ''' + table_name + ''' (''' + columns + ''') VALUES (''' + '''?,'''*(number_of_columns-1) + '''?)'''\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframes(file):\n",
    "    '''Takes a csv file, converts it to Pandas Dataframe and Loads it into a Table in SQL Server.\n",
    "    \n",
    "        Args: Takes a file as an argument.\n",
    "    '''\n",
    "    path = r'C:\\OlistPrimaryDataSet\\\\'\n",
    "\n",
    "    table_name = (file.split(\".\")[0]).upper()\n",
    "    print(\"Inserting data into \" + table_name + \" Table\")\n",
    "\n",
    "    dataframe = pd.read_csv(path+file)\n",
    "    dataframe = dataframe.drop_duplicates()\n",
    "    dataframe = dataframe.astype(str)\n",
    "\n",
    "    # creating column list for insertion\n",
    "    cols = \",\".join([str(i) for i in dataframe.columns.tolist()])\n",
    "    query = construct_query(table_name, cols, len(cols.split(\",\")))\n",
    "\n",
    "    #Inserting data into table in chunks\n",
    "    for subset in chunker(dataframe.values.tolist(), 1000):\n",
    "        cursor.execute(query, tuple(row))\n",
    "    \n",
    "    # the connection is not autocommitted by default, so we must commit to save our changes\n",
    "    connection.commit()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establishing a DB Connection an creating Tables for Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_str = 'Driver={SQL Server};' + 'Server=DESKTOP-K4UDQKF;' + 'Database=OLISTSTORE_DB;' + 'Trusted_Connection=yes;'\n",
    "connection = pyodbc.connect(connection_str)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.fast_executemany = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data into the tables in the order of dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data into OLIST_ORDER_REVIEWS Table\n"
     ]
    }
   ],
   "source": [
    "order = base_tbls+intermediate_tbls+dependent_tbls\n",
    "\n",
    "for item in order:\n",
    "    load_dataframes(item)\n",
    "\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
